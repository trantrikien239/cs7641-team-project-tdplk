{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98fdbf2d",
   "metadata": {},
   "source": [
    "### Data Preparation Outcomes:\n",
    "Varaibles created and their description\n",
    "\n",
    "* word_token_nltk: Raw data word tokens (Punctuations are separate tokens)\n",
    "* sent_token: Rarw data sentence tokens\n",
    "* word_token_manual: Raw data word tokens( Punctuations are not tokens)\n",
    "* clean_text: Text data with no stop words and punctuations\n",
    "* lemm_text: Clean text with lemmatization\n",
    "* freq_dist: Dictionary of lemm words with their frequency counts\n",
    "* most_common_words: List of tuple of 3 most frequently occuring lemmatized text words with their count\n",
    "* distinct_words_cnt: count of distinct lemmatized text words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9013b8dd",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e21a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5532abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "# import nltk\n",
    "# nltk.download('all')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c81782",
   "metadata": {},
   "source": [
    "#### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd5b0bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>When a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>The best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>Small act of kindness can impact in other peop...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  syntax  vocabulary  phraseology  grammar  conventions\n",
       "0  0016926B079C  I think that students would benefit from learn...       3.5     3.5         3.0          3.0      4.0          3.0\n",
       "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5     2.5         3.0          2.0      2.0          2.5\n",
       "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0     3.5         3.0          3.0      3.0          2.5\n",
       "3  003885A45F42  The best time in life is when you become yours...       4.5     4.5         4.5          4.5      4.0          5.0\n",
       "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5     3.0         3.0          3.0      2.5          2.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train_data/train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea55ee31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3911 entries, 0 to 3910\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   text_id      3911 non-null   object \n",
      " 1   full_text    3911 non-null   object \n",
      " 2   cohesion     3911 non-null   float64\n",
      " 3   syntax       3911 non-null   float64\n",
      " 4   vocabulary   3911 non-null   float64\n",
      " 5   phraseology  3911 non-null   float64\n",
      " 6   grammar      3911 non-null   float64\n",
      " 7   conventions  3911 non-null   float64\n",
      "dtypes: float64(6), object(2)\n",
      "memory usage: 244.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f4f49d",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "085033d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['word_token_nltk'] = data['full_text'].apply(lambda x: word_tokenize(x)) #Tokenize each words and punctuations\n",
    "data['sent_token'] = data['full_text'].apply(lambda x: sent_tokenize(x))# Tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4933b27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'think', 'that', 'students', 'would', 'benefit', 'from', 'learning', 'at', 'home,because', 'they', 'wont', 'have', 'to', 'change', 'and', 'get', 'up', 'early', 'in', 'the', 'morning', 'to', 'shower', 'and', 'do', 'there', 'hair.', 'taking', 'only', 'classes', 'helps', 'them', 'because', 'at', 'there', 'house', \"they'll\", 'be', 'pay', 'more', 'attention.', 'they', 'will', 'be', 'comfortable', 'at', 'home.\\n\\nThe', 'hardest', 'part', 'of', 'school', 'is', 'getting', 'ready.', 'you', 'wake', 'up', 'go', 'brush', 'your', 'teeth', 'and', 'go', 'to', 'your', 'closet', 'and', 'look', 'at', 'your', 'cloths.', 'after', 'you', 'think', 'you', 'picked', 'a', 'outfit', 'u', 'go', 'look', 'in', 'the', 'mirror', 'and', 'youll', 'either', 'not', 'like', 'it', 'or', 'you', 'look', 'and', 'see', 'a', 'stain.', 'Then', \"you'll\", 'have', 'to', 'change.', 'with', 'the', 'online', 'classes', 'you', 'can', 'wear', 'anything', 'and', 'stay', 'home', 'and', 'you', 'wont', 'need', 'to', 'stress', 'about', 'what', 'to', 'wear.\\n\\nmost', 'students', 'usually', 'take', 'showers', 'before', 'school.', 'they', 'either', 'take', 'it', 'before', 'they', 'sleep', 'or', 'when', 'they', 'wake', 'up.', 'some', 'students', 'do', 'both', 'to', 'smell', 'good.', 'that', 'causes', 'them', 'do', 'miss', 'the', 'bus', 'and', 'effects', 'on', 'there', 'lesson', 'time', 'cause', 'they', 'come', 'late', 'to', 'school.', 'when', 'u', 'have', 'online', 'classes', 'u', 'wont', 'need', 'to', 'miss', 'lessons', 'cause', 'you', 'can', 'get', 'everything', 'set', 'up', 'and', 'go', 'take', 'a', 'shower', 'and', 'when', 'u', 'get', 'out', 'your', 'ready', 'to', 'go.\\n\\nwhen', 'your', 'home', 'your', 'comfortable', 'and', 'you', 'pay', 'attention.', 'it', 'gives', 'then', 'an', 'advantage', 'to', 'be', 'smarter', 'and', 'even', 'pass', 'there', 'classmates', 'on', 'class', 'work.', 'public', 'schools', 'are', 'difficult', 'even', 'if', 'you', 'try.', 'some', 'teacher', 'dont', 'know', 'how', 'to', 'teach', 'it', 'in', 'then', 'way', 'that', 'students', 'understand', 'it.', 'that', 'causes', 'students', 'to', 'fail', 'and', 'they', 'may', 'repeat', 'the', 'class.', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "# Splitting using space should preserve context in terms of use of punctuation\n",
    "data['word_token_manual'] =data['full_text'].apply(lambda x: x.split(' '))\n",
    "print(data['word_token_manual'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5026b075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'think', 'that', 'students', 'would', 'benefit', 'from', 'learning', 'at', 'home', ',', 'because', 'they', 'wont', 'have', 'to', 'change', 'and', 'get', 'up', 'early', 'in', 'the', 'morning', 'to', 'shower', 'and', 'do', 'there', 'hair', '.', 'taking', 'only', 'classes', 'helps', 'them', 'because', 'at', 'there', 'house', 'they', \"'ll\", 'be', 'pay', 'more', 'attention', '.', 'they', 'will', 'be', 'comfortable', 'at', 'home', '.', 'The', 'hardest', 'part', 'of', 'school', 'is', 'getting', 'ready', '.', 'you', 'wake', 'up', 'go', 'brush', 'your', 'teeth', 'and', 'go', 'to', 'your', 'closet', 'and', 'look', 'at', 'your', 'cloths', '.', 'after', 'you', 'think', 'you', 'picked', 'a', 'outfit', 'u', 'go', 'look', 'in', 'the', 'mirror', 'and', 'youll', 'either', 'not', 'like', 'it', 'or', 'you', 'look', 'and', 'see', 'a', 'stain', '.', 'Then', 'you', \"'ll\", 'have', 'to', 'change', '.', 'with', 'the', 'online', 'classes', 'you', 'can', 'wear', 'anything', 'and', 'stay', 'home', 'and', 'you', 'wont', 'need', 'to', 'stress', 'about', 'what', 'to', 'wear', '.', 'most', 'students', 'usually', 'take', 'showers', 'before', 'school', '.', 'they', 'either', 'take', 'it', 'before', 'they', 'sleep', 'or', 'when', 'they', 'wake', 'up', '.', 'some', 'students', 'do', 'both', 'to', 'smell', 'good', '.', 'that', 'causes', 'them', 'do', 'miss', 'the', 'bus', 'and', 'effects', 'on', 'there', 'lesson', 'time', 'cause', 'they', 'come', 'late', 'to', 'school', '.', 'when', 'u', 'have', 'online', 'classes', 'u', 'wont', 'need', 'to', 'miss', 'lessons', 'cause', 'you', 'can', 'get', 'everything', 'set', 'up', 'and', 'go', 'take', 'a', 'shower', 'and', 'when', 'u', 'get', 'out', 'your', 'ready', 'to', 'go', '.', 'when', 'your', 'home', 'your', 'comfortable', 'and', 'you', 'pay', 'attention', '.', 'it', 'gives', 'then', 'an', 'advantage', 'to', 'be', 'smarter', 'and', 'even', 'pass', 'there', 'classmates', 'on', 'class', 'work', '.', 'public', 'schools', 'are', 'difficult', 'even', 'if', 'you', 'try', '.', 'some', 'teacher', 'dont', 'know', 'how', 'to', 'teach', 'it', 'in', 'then', 'way', 'that', 'students', 'understand', 'it', '.', 'that', 'causes', 'students', 'to', 'fail', 'and', 'they', 'may', 'repeat', 'the', 'class', '.']\n",
      "['I think that students would benefit from learning at home,because they wont have to change and get up early in the morning to shower and do there hair.', \"taking only classes helps them because at there house they'll be pay more attention.\", 'they will be comfortable at home.', 'The hardest part of school is getting ready.', 'you wake up go brush your teeth and go to your closet and look at your cloths.', 'after you think you picked a outfit u go look in the mirror and youll either not like it or you look and see a stain.', \"Then you'll have to change.\", 'with the online classes you can wear anything and stay home and you wont need to stress about what to wear.', 'most students usually take showers before school.', 'they either take it before they sleep or when they wake up.', 'some students do both to smell good.', 'that causes them do miss the bus and effects on there lesson time cause they come late to school.', 'when u have online classes u wont need to miss lessons cause you can get everything set up and go take a shower and when u get out your ready to go.', 'when your home your comfortable and you pay attention.', 'it gives then an advantage to be smarter and even pass there classmates on class work.', 'public schools are difficult even if you try.', 'some teacher dont know how to teach it in then way that students understand it.', 'that causes students to fail and they may repeat the class.']\n"
     ]
    }
   ],
   "source": [
    "print(data['word_token_nltk'][0])#Sample\n",
    "print(data['sent_token'][0])#Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f3936f",
   "metadata": {},
   "source": [
    "#### tokenization and features on clean data(Without stop words and punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "419690d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['think', 'student', 'would', 'benefit', 'learning', 'home', 'wont', 'change', 'get', 'early', 'morning', 'shower', 'hair', 'taking', 'class', 'help', 'house', 'pay', 'attention', 'comfortable', 'home', 'hardest', 'part', 'school', 'getting', 'ready', 'wake', 'go', 'brush', 'teeth', 'go', 'closet', 'look', 'cloth', 'think', 'picked', 'outfit', 'u', 'go', 'look', 'mirror', 'youll', 'either', 'like', 'look', 'see', 'stain', 'change', 'online', 'class', 'wear', 'anything', 'stay', 'home', 'wont', 'need', 'stress', 'wear', 'student', 'usually', 'take', 'shower', 'school', 'either', 'take', 'sleep', 'wake', 'student', 'smell', 'good', 'cause', 'miss', 'bus', 'effect', 'lesson', 'time', 'cause', 'come', 'late', 'school', 'u', 'online', 'class', 'u', 'wont', 'need', 'miss', 'lesson', 'cause', 'get', 'everything', 'set', 'go', 'take', 'shower', 'u', 'get', 'ready', 'go', 'home', 'comfortable', 'pay', 'attention', 'give', 'advantage', 'smarter', 'even', 'pas', 'classmate', 'class', 'work', 'public', 'school', 'difficult', 'even', 'try', 'teacher', 'dont', 'know', 'teach', 'way', 'student', 'understand', 'cause', 'student', 'fail', 'may', 'repeat', 'class']\n"
     ]
    }
   ],
   "source": [
    "#Creating a stopword free full text column for analysis; Punctuations need to be removed separately\n",
    "stopword_list = stopwords.words(\"english\")\n",
    "data['clean_text'] = data['full_text'].apply(lambda x: [i.lower() for i in word_tokenize(x) if i.lower() not in stopword_list])\n",
    "data['clean_text'] = data['clean_text'].apply(lambda x: [w for w in x if w.isalpha()])\n",
    "# Lemmatizing the tokens to convert them to contextual root words\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "data['lemm_text'] = data['clean_text'].apply(lambda x: [wordnet_lemmatizer.lemmatize(w) for w in x])\n",
    "print(data['lemm_text'][0])#Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5208cb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'student': 5, 'class': 5, 'go': 5, 'home': 4, 'school': 4, 'u': 4, 'cause': 4, 'wont': 3, 'get': 3, 'shower': 3, ...})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['freq_dist'] = data['lemm_text'].apply(lambda x: FreqDist(x))\n",
    "data['freq_dist'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ac42a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 81 samples and 129 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print(data['freq_dist'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59457424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('student', 5), ('class', 5), ('go', 5)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of 3 most commonly used words with their count\n",
    "data['most_common_words'] = data['freq_dist'].apply(lambda x: x.most_common(3))\n",
    "data['most_common_words'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "755186da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distinct root words in the clean data\n",
    "data['distinct_words_cnt'] = data['freq_dist'].apply(lambda x: len(x.keys()))\n",
    "data['distinct_words_cnt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "579aa3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing Data to csv\n",
    "data.to_csv('train_tokenized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59b481a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
