{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSy0vgnpgS-o",
        "outputId": "a91a6c9f-f255-4a5a-f1ab-5ceba34dc096"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "L1nHQXile4w-"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim.downloader\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "D5xNLYaJe4xA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db0b3e8b-f2dd-4773-c649-a08dd3248d75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oNjswtI4e4xB"
      },
      "outputs": [],
      "source": [
        "from dataset import EssayDataset, essay_collate_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4k1EsREKe4xB"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "np.random.seed(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IWm_-Aqqe4xB",
        "outputId": "7e696279-e6b4-4eff-9fc5-6f8f56cc6735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        text_id                                          full_text  cohesion  \\\n",
              "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
              "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
              "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n",
              "3  003885A45F42  The best time in life is when you become yours...       4.5   \n",
              "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n",
              "\n",
              "   syntax  vocabulary  phraseology  grammar  conventions  \\\n",
              "0     3.5         3.0          3.0      4.0          3.0   \n",
              "1     2.5         3.0          2.0      2.0          2.5   \n",
              "2     3.5         3.0          3.0      3.0          2.5   \n",
              "3     4.5         4.5          4.5      4.0          5.0   \n",
              "4     3.0         3.0          3.0      2.5          2.5   \n",
              "\n",
              "                                     word_token_nltk  \\\n",
              "0  ['I', 'think', 'that', 'students', 'would', 'b...   \n",
              "1  ['When', 'a', 'problem', 'is', 'a', 'change', ...   \n",
              "2  ['Dear', ',', 'Principal', 'If', 'u', 'change'...   \n",
              "3  ['The', 'best', 'time', 'in', 'life', 'is', 'w...   \n",
              "4  ['Small', 'act', 'of', 'kindness', 'can', 'imp...   \n",
              "\n",
              "                                          sent_token  \\\n",
              "0  ['I think that students would benefit from lea...   \n",
              "1  ['When a problem is a change you have to let i...   \n",
              "2  ['Dear, Principal\\n\\nIf u change the school po...   \n",
              "3  ['The best time in life is when you become you...   \n",
              "4  ['Small act of kindness can impact in other pe...   \n",
              "\n",
              "                                   word_token_manual  \\\n",
              "0  ['I', 'think', 'that', 'students', 'would', 'b...   \n",
              "1  ['When', 'a', 'problem', 'is', 'a', 'change', ...   \n",
              "2  ['Dear,', 'Principal\\n\\nIf', 'u', 'change', 't...   \n",
              "3  ['The', 'best', 'time', 'in', 'life', 'is', 'w...   \n",
              "4  ['Small', 'act', 'of', 'kindness', 'can', 'imp...   \n",
              "\n",
              "                                          clean_text  \\\n",
              "0  ['think', 'students', 'would', 'benefit', 'lea...   \n",
              "1  ['problem', 'change', 'let', 'best', 'matter',...   \n",
              "2  ['dear', 'principal', 'u', 'change', 'school',...   \n",
              "3  ['best', 'time', 'life', 'become', 'agree', 'g...   \n",
              "4  ['small', 'act', 'kindness', 'impact', 'people...   \n",
              "\n",
              "                                           lemm_text  \\\n",
              "0  ['think', 'student', 'would', 'benefit', 'lear...   \n",
              "1  ['problem', 'change', 'let', 'best', 'matter',...   \n",
              "2  ['dear', 'principal', 'u', 'change', 'school',...   \n",
              "3  ['best', 'time', 'life', 'become', 'agree', 'g...   \n",
              "4  ['small', 'act', 'kindness', 'impact', 'people...   \n",
              "\n",
              "                                      freq_dist  \\\n",
              "0   <FreqDist with 81 samples and 129 outcomes>   \n",
              "1   <FreqDist with 80 samples and 215 outcomes>   \n",
              "2   <FreqDist with 58 samples and 133 outcomes>   \n",
              "3  <FreqDist with 132 samples and 282 outcomes>   \n",
              "4   <FreqDist with 67 samples and 112 outcomes>   \n",
              "\n",
              "                                   most_common_words  distinct_words_cnt  \n",
              "0          [('student', 5), ('class', 5), ('go', 5)]                  81  \n",
              "1  [('change', 16), ('different', 12), ('problem'...                  80  \n",
              "2      [('school', 9), ('average', 9), ('sport', 8)]                  58  \n",
              "3      [('make', 16), ('choice', 10), ('others', 8)]                 132  \n",
              "4         [('people', 6), ('person', 6), ('act', 5)]                  67  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-34fda0de-d2b6-4563-9551-06b0f16fd2b4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>full_text</th>\n",
              "      <th>cohesion</th>\n",
              "      <th>syntax</th>\n",
              "      <th>vocabulary</th>\n",
              "      <th>phraseology</th>\n",
              "      <th>grammar</th>\n",
              "      <th>conventions</th>\n",
              "      <th>word_token_nltk</th>\n",
              "      <th>sent_token</th>\n",
              "      <th>word_token_manual</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>lemm_text</th>\n",
              "      <th>freq_dist</th>\n",
              "      <th>most_common_words</th>\n",
              "      <th>distinct_words_cnt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0016926B079C</td>\n",
              "      <td>I think that students would benefit from learn...</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>['I', 'think', 'that', 'students', 'would', 'b...</td>\n",
              "      <td>['I think that students would benefit from lea...</td>\n",
              "      <td>['I', 'think', 'that', 'students', 'would', 'b...</td>\n",
              "      <td>['think', 'students', 'would', 'benefit', 'lea...</td>\n",
              "      <td>['think', 'student', 'would', 'benefit', 'lear...</td>\n",
              "      <td>&lt;FreqDist with 81 samples and 129 outcomes&gt;</td>\n",
              "      <td>[('student', 5), ('class', 5), ('go', 5)]</td>\n",
              "      <td>81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0022683E9EA5</td>\n",
              "      <td>When a problem is a change you have to let it ...</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>['When', 'a', 'problem', 'is', 'a', 'change', ...</td>\n",
              "      <td>['When a problem is a change you have to let i...</td>\n",
              "      <td>['When', 'a', 'problem', 'is', 'a', 'change', ...</td>\n",
              "      <td>['problem', 'change', 'let', 'best', 'matter',...</td>\n",
              "      <td>['problem', 'change', 'let', 'best', 'matter',...</td>\n",
              "      <td>&lt;FreqDist with 80 samples and 215 outcomes&gt;</td>\n",
              "      <td>[('change', 16), ('different', 12), ('problem'...</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00299B378633</td>\n",
              "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>['Dear', ',', 'Principal', 'If', 'u', 'change'...</td>\n",
              "      <td>['Dear, Principal\\n\\nIf u change the school po...</td>\n",
              "      <td>['Dear,', 'Principal\\n\\nIf', 'u', 'change', 't...</td>\n",
              "      <td>['dear', 'principal', 'u', 'change', 'school',...</td>\n",
              "      <td>['dear', 'principal', 'u', 'change', 'school',...</td>\n",
              "      <td>&lt;FreqDist with 58 samples and 133 outcomes&gt;</td>\n",
              "      <td>[('school', 9), ('average', 9), ('sport', 8)]</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>003885A45F42</td>\n",
              "      <td>The best time in life is when you become yours...</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>['The', 'best', 'time', 'in', 'life', 'is', 'w...</td>\n",
              "      <td>['The best time in life is when you become you...</td>\n",
              "      <td>['The', 'best', 'time', 'in', 'life', 'is', 'w...</td>\n",
              "      <td>['best', 'time', 'life', 'become', 'agree', 'g...</td>\n",
              "      <td>['best', 'time', 'life', 'become', 'agree', 'g...</td>\n",
              "      <td>&lt;FreqDist with 132 samples and 282 outcomes&gt;</td>\n",
              "      <td>[('make', 16), ('choice', 10), ('others', 8)]</td>\n",
              "      <td>132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0049B1DF5CCC</td>\n",
              "      <td>Small act of kindness can impact in other peop...</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>['Small', 'act', 'of', 'kindness', 'can', 'imp...</td>\n",
              "      <td>['Small act of kindness can impact in other pe...</td>\n",
              "      <td>['Small', 'act', 'of', 'kindness', 'can', 'imp...</td>\n",
              "      <td>['small', 'act', 'kindness', 'impact', 'people...</td>\n",
              "      <td>['small', 'act', 'kindness', 'impact', 'people...</td>\n",
              "      <td>&lt;FreqDist with 67 samples and 112 outcomes&gt;</td>\n",
              "      <td>[('people', 6), ('person', 6), ('act', 5)]</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34fda0de-d2b6-4563-9551-06b0f16fd2b4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-34fda0de-d2b6-4563-9551-06b0f16fd2b4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-34fda0de-d2b6-4563-9551-06b0f16fd2b4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data = pd.read_csv(\"drive/MyDrive/data/cs7641/train_tokenized.csv\") # Palash's file\n",
        "data.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SArq5H-_e4xC",
        "outputId": "94da335e-7093-4765-dd83-3baa310b4781",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 252.1/252.1MB downloaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-200')\n",
        "glove_dict = glove_vectors.key_to_index\n",
        "len(glove_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q7yft7n_e4xC"
      },
      "outputs": [],
      "source": [
        "gensim_emb_weights = glove_vectors.vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FbTPz6fYe4xC"
      },
      "outputs": [],
      "source": [
        "data['list_lower_word_token_nltk'] = data['word_token_nltk'].apply(eval)\n",
        "data['list_lower_word_token_nltk'] = data['list_lower_word_token_nltk'].apply(lambda l: [e.lower() for e in l if e not in ('', ' ')])\n",
        "data['num_valid_token'] = data['list_lower_word_token_nltk'].apply(lambda x: len([c for c in x if c in glove_dict]))\n",
        "data['num_all_token'] = data['list_lower_word_token_nltk'].apply(len)\n",
        "data['unrecgonized_tokens'] = data['list_lower_word_token_nltk'].apply(lambda x: [c for c in x if c not in glove_dict])\n",
        "data[\"word_token_nltk_idx\"] = data['list_lower_word_token_nltk'].apply(\n",
        "    lambda x: [glove_vectors.key_to_index[c]  if c in glove_dict else 400000 for c in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "U9lUANBme4xD",
        "outputId": "bd4aff34-fc41-4cb8-e31c-5d6051ae9294",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [41, 269, 12, 543, 54, 2161, 25, 2741, 22, 163...\n",
              "1       [61, 7, 747, 14, 7, 511, 81, 33, 4, 886, 20, 8...\n",
              "2       [8607, 1, 2965, 83, 6479, 511, 0, 164, 527, 3,...\n",
              "3       [0, 254, 79, 6, 214, 14, 61, 81, 399, 4961, 2,...\n",
              "4       [357, 743, 3, 20524, 86, 1433, 6, 68, 69, 86, ...\n",
              "                              ...                        \n",
              "3906    [41, 733, 622, 28319, 6, 905, 10, 631, 95, 91,...\n",
              "3907    [500, 1485, 1, 543, 88, 36, 33, 4, 4117, 17, 6...\n",
              "3908    [28, 7, 747, 14, 7, 1019, 10, 81, 4, 88, 392, ...\n",
              "3909    [109, 69, 10027, 17, 4324, 28833, 9, 8643, 1, ...\n",
              "3910    [88, 81, 269, 12, 1899, 14, 0, 444, 873, 10, 6...\n",
              "Name: word_token_nltk_idx, Length: 3911, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "data[\"word_token_nltk_idx\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vVRNiqAHe4xD",
        "outputId": "249f0d4e-801c-47cb-cc52-d5a34809f74d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.5, 4.5, 4.5, 4.5, 4.0, 5.0]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "task_cols = [\"cohesion\",\"syntax\",\"vocabulary\", \n",
        "    \"phraseology\", \"grammar\", \"conventions\"]\n",
        "data.loc[3, task_cols].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wQxgCfQwe4xE"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "df_train, df_valid, df_test = np.split(\n",
        "    data[[\"text_id\", \"full_text\", \"word_token_nltk_idx\"] + task_cols].sample(\n",
        "        frac=1, random_state=42), \n",
        "    [int(.8*len(data)), int(.9*len(data))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Zi6Z1zbue4xE",
        "outputId": "d90300ed-d005-47e5-a99e-4ccf45f1e170",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           text_id                                          full_text  \\\n",
              "1552  772D27D400BB  It god to have a possitive attitude when you d...   \n",
              "2114  9E8F3C6405CA  Why do people ask more then one person for adv...   \n",
              "1965  948771F795EB  We accomplish more when we are active, and are...   \n",
              "3856  FE14D7378CFB  Do you agree or disagree about imagination bei...   \n",
              "1610  7AAE019F70D6  I disagree with the principal saying that all ...   \n",
              "\n",
              "                                    word_token_nltk_idx  cohesion  syntax  \\\n",
              "1552  [20, 1533, 4, 33, 7, 400000, 4191, 61, 81, 88,...       3.0     2.5   \n",
              "2114  [738, 88, 69, 1712, 56, 127, 48, 899, 10, 3240...       3.0     2.0   \n",
              "1965  [53, 9749, 56, 61, 53, 32, 1546, 1, 5, 32, 690...       4.0     4.0   \n",
              "3856  [88, 81, 2137, 46, 10027, 59, 9201, 134, 56, 4...       3.0     3.0   \n",
              "1610  [41, 10027, 17, 0, 2965, 345, 12, 64, 1813, 18...       3.5     3.5   \n",
              "\n",
              "      vocabulary  phraseology  grammar  conventions  \n",
              "1552         2.5          2.0      2.0          2.0  \n",
              "2114         3.0          3.5      3.0          3.0  \n",
              "1965         3.0          4.0      4.0          4.0  \n",
              "3856         3.5          3.0      3.5          3.5  \n",
              "1610         3.5          3.5      3.0          3.5  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-af678c71-492d-4683-b6e5-8864d2e56245\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_id</th>\n",
              "      <th>full_text</th>\n",
              "      <th>word_token_nltk_idx</th>\n",
              "      <th>cohesion</th>\n",
              "      <th>syntax</th>\n",
              "      <th>vocabulary</th>\n",
              "      <th>phraseology</th>\n",
              "      <th>grammar</th>\n",
              "      <th>conventions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1552</th>\n",
              "      <td>772D27D400BB</td>\n",
              "      <td>It god to have a possitive attitude when you d...</td>\n",
              "      <td>[20, 1533, 4, 33, 7, 400000, 4191, 61, 81, 88,...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2114</th>\n",
              "      <td>9E8F3C6405CA</td>\n",
              "      <td>Why do people ask more then one person for adv...</td>\n",
              "      <td>[738, 88, 69, 1712, 56, 127, 48, 899, 10, 3240...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1965</th>\n",
              "      <td>948771F795EB</td>\n",
              "      <td>We accomplish more when we are active, and are...</td>\n",
              "      <td>[53, 9749, 56, 61, 53, 32, 1546, 1, 5, 32, 690...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3856</th>\n",
              "      <td>FE14D7378CFB</td>\n",
              "      <td>Do you agree or disagree about imagination bei...</td>\n",
              "      <td>[88, 81, 2137, 46, 10027, 59, 9201, 134, 56, 4...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1610</th>\n",
              "      <td>7AAE019F70D6</td>\n",
              "      <td>I disagree with the principal saying that all ...</td>\n",
              "      <td>[41, 10027, 17, 0, 2965, 345, 12, 64, 1813, 18...</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af678c71-492d-4683-b6e5-8864d2e56245')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-af678c71-492d-4683-b6e5-8864d2e56245 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-af678c71-492d-4683-b6e5-8864d2e56245');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LOWEqxXSe4xE"
      },
      "outputs": [],
      "source": [
        "ds_train = EssayDataset(df_train, \"word_token_nltk_idx\", task_cols)\n",
        "dl_train = torch.utils.data.DataLoader(\n",
        "    ds_train, batch_size=BATCH_SIZE, \n",
        "    shuffle=True, collate_fn=essay_collate_fn)\n",
        "ds_valid = EssayDataset(df_valid, \"word_token_nltk_idx\", task_cols)\n",
        "dl_valid = torch.utils.data.DataLoader(\n",
        "    ds_valid, batch_size=BATCH_SIZE, \n",
        "    shuffle=True, collate_fn=essay_collate_fn)\n",
        "ds_test = EssayDataset(df_test, \"word_token_nltk_idx\", task_cols)\n",
        "dl_test = torch.utils.data.DataLoader(\n",
        "    ds_test, batch_size=BATCH_SIZE, \n",
        "    shuffle=True, collate_fn=essay_collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.to_parquet(\"drive/MyDrive/data/cs7641/df_train.pq\")\n",
        "df_valid.to_parquet(\"drive/MyDrive/data/cs7641/df_valid.pq\")\n",
        "df_test.to_parquet(\"drive/MyDrive/data/cs7641/df_test.pq\")"
      ],
      "metadata": {
        "id": "JfLoWAKqf3N_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YPcDRoFSe4xF"
      },
      "outputs": [],
      "source": [
        "from gru import GRUGrader\n",
        "from trainer import train_grader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5TbqF30Fe4xF"
      },
      "outputs": [],
      "source": [
        "gru_sizes = [32, 64, 128]\n",
        "gru_depth = [4, 6, 8]\n",
        "bidirectional = [True, False]\n",
        "decoder_depth = [2,3,4]\n",
        "decoder_size = {\n",
        "    2: [256, 64],\n",
        "    3: [256, 512, 64],\n",
        "    4: [256, 512, 128, 64]\n",
        "}\n",
        "\n",
        "model_versions = {\n",
        "    # Default\n",
        "    \"gru_64_6__bidirectiona_True__decoder_3\":{\n",
        "        \"gru_size\":64,\n",
        "        \"gru_num_layer\":6,\n",
        "        \"bidirectional\":True,\n",
        "        \"decoder_depth\":3,\n",
        "        \"decoder_size\":[256, 512, 64]\n",
        "    },\n",
        "    # Experiment with gru size\n",
        "    \"gru_32_6__bidirectiona_True__decoder_3\":{\n",
        "        \"gru_size\":32,\n",
        "        \"gru_num_layer\":6,\n",
        "        \"bidirectional\":True,\n",
        "        \"decoder_depth\":3,\n",
        "        \"decoder_size\":[256, 512, 64]\n",
        "    },\n",
        "    \"gru_128_6__bidirectiona_True__decoder_3\":{\n",
        "        \"gru_size\":128,\n",
        "        \"gru_num_layer\":6,\n",
        "        \"bidirectional\":True,\n",
        "        \"decoder_depth\":3,\n",
        "        \"decoder_size\":[256, 512, 64]\n",
        "    },\n",
        "    # Experiment with gru depth\n",
        "    \"gru_64_4__bidirectiona_True__decoder_3\":{\n",
        "        \"gru_size\":64,\n",
        "        \"gru_num_layer\":4,\n",
        "        \"bidirectional\":True,\n",
        "        \"decoder_depth\":3,\n",
        "        \"decoder_size\":[256, 512, 64]\n",
        "    },\n",
        "    \"gru_64_8__bidirectiona_True__decoder_3\":{\n",
        "        \"gru_size\":64,\n",
        "        \"gru_num_layer\":8,\n",
        "        \"bidirectional\":True,\n",
        "        \"decoder_depth\":3,\n",
        "        \"decoder_size\":[256, 512, 64]\n",
        "    },\n",
        "    # Experiment with bidirectional = False\n",
        "    \"gru_64_6__bidirectiona_False__decoder_3\":{\n",
        "        \"gru_size\":64,\n",
        "        \"gru_num_layer\":6,\n",
        "        \"bidirectional\":False,\n",
        "        \"decoder_depth\":3,\n",
        "        \"decoder_size\":[256, 512, 64]\n",
        "    },\n",
        "    # Experiment with decoder depth\n",
        "    \"gru_64_6__bidirectiona_True__decoder_2\":{\n",
        "        \"gru_size\":64,\n",
        "        \"gru_num_layer\":6,\n",
        "        \"bidirectional\":True,\n",
        "        \"decoder_depth\":2,\n",
        "        \"decoder_size\":[256,64]\n",
        "    },\n",
        "    \"gru_64_6__bidirectiona_True__decoder_4\":{\n",
        "        \"gru_size\":64,\n",
        "        \"gru_num_layer\":6,\n",
        "        \"bidirectional\":True,\n",
        "        \"decoder_depth\":4,\n",
        "        \"decoder_size\":[256, 512, 128, 64]\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SRwMzeVBe4xF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sZohz8ode4xF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc35f0a8-e01c-4f7c-c9c2-70d9f275497d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gru_64_6__bidirectiona_True__decoder_3\n",
            "Epoch 0000, train loss: 0.592711, valid loss: 0.584031, best valid loss: inf\n",
            "Epoch 0001, train loss: 0.430979, valid loss: 0.384714, best valid loss: inf\n",
            "Epoch 0002, train loss: 0.418822, valid loss: 0.406971, best valid loss: inf\n",
            "Epoch 0003, train loss: 0.410147, valid loss: 0.373027, best valid loss: inf\n",
            "Epoch 0004, train loss: 0.396715, valid loss: 0.352475, best valid loss: inf\n",
            "Epoch 0005, train loss: 0.376862, valid loss: 0.336481, best valid loss: inf\n",
            "Epoch 0006, train loss: 0.334355, valid loss: 0.324591, best valid loss: inf\n",
            "Epoch 0007, train loss: 0.338426, valid loss: 0.343952, best valid loss: 0.343952\n",
            "Epoch 0008, train loss: 0.305288, valid loss: 0.298866, best valid loss: 0.343952\n",
            "Epoch 0009, train loss: 0.289566, valid loss: 0.315413, best valid loss: 0.315413\n",
            "Epoch 0010, train loss: 0.277197, valid loss: 0.314843, best valid loss: 0.314843\n",
            "Epoch 0011, train loss: 0.277565, valid loss: 0.322955, best valid loss: 0.314843\n",
            "Epoch 0012, train loss: 0.275343, valid loss: 0.397711, best valid loss: 0.314843\n",
            "Epoch 0013, train loss: 0.297842, valid loss: 0.354254, best valid loss: 0.314843\n",
            "Epoch 0014, train loss: 0.264184, valid loss: 0.308969, best valid loss: 0.308969\n",
            "Epoch 0015, train loss: 0.255805, valid loss: 0.347841, best valid loss: 0.308969\n",
            "Epoch 0016, train loss: 0.230024, valid loss: 0.352792, best valid loss: 0.308969\n",
            "Epoch 0017, train loss: 0.223350, valid loss: 0.323360, best valid loss: 0.308969\n",
            "Epoch 0018, train loss: 0.208548, valid loss: 0.324292, best valid loss: 0.308969\n",
            "Epoch 0019, train loss: 0.249415, valid loss: 0.400273, best valid loss: 0.308969\n",
            "Epoch 0020, train loss: 0.202115, valid loss: 0.360103, best valid loss: 0.308969\n",
            "Epoch 0021, train loss: 0.188936, valid loss: 0.324300, best valid loss: 0.308969\n",
            "Epoch 0022, train loss: 0.180404, valid loss: 0.349383, best valid loss: 0.308969\n",
            "Epoch 0023, train loss: 0.170934, valid loss: 0.386689, best valid loss: 0.308969\n",
            "Epoch 0024, train loss: 0.171124, valid loss: 0.397001, best valid loss: 0.308969\n",
            "Early stopping, best valid loss: 0.308969\n",
            "--------------\n",
            "gru_32_6__bidirectiona_True__decoder_3\n",
            "Epoch 0000, train loss: 0.829074, valid loss: 0.680084, best valid loss: inf\n",
            "Epoch 0001, train loss: 0.425732, valid loss: 0.384149, best valid loss: inf\n",
            "Epoch 0002, train loss: 0.416831, valid loss: 0.383316, best valid loss: inf\n",
            "Epoch 0003, train loss: 0.412249, valid loss: 0.441801, best valid loss: 0.441801\n",
            "Epoch 0004, train loss: 0.412397, valid loss: 0.456654, best valid loss: 0.441801\n",
            "Epoch 0005, train loss: 0.394204, valid loss: 0.406257, best valid loss: 0.406257\n",
            "Epoch 0006, train loss: 0.380180, valid loss: 0.378406, best valid loss: 0.406257\n",
            "Epoch 0007, train loss: 0.365935, valid loss: 0.345430, best valid loss: 0.406257\n",
            "Epoch 0008, train loss: 0.347226, valid loss: 0.328620, best valid loss: 0.406257\n",
            "Epoch 0009, train loss: 0.330832, valid loss: 0.336290, best valid loss: 0.336290\n",
            "Epoch 0010, train loss: 0.304264, valid loss: 0.312407, best valid loss: 0.312407\n",
            "Epoch 0011, train loss: 0.303287, valid loss: 0.402478, best valid loss: 0.312407\n",
            "Epoch 0012, train loss: 0.283781, valid loss: 0.324895, best valid loss: 0.312407\n",
            "Epoch 0013, train loss: 0.283020, valid loss: 0.325065, best valid loss: 0.312407\n",
            "Epoch 0014, train loss: 0.261420, valid loss: 0.402941, best valid loss: 0.312407\n",
            "Epoch 0015, train loss: 0.248946, valid loss: 0.352551, best valid loss: 0.312407\n",
            "Epoch 0016, train loss: 0.245634, valid loss: 0.435556, best valid loss: 0.312407\n",
            "Epoch 0017, train loss: 0.263365, valid loss: 0.467559, best valid loss: 0.312407\n",
            "Epoch 0018, train loss: 0.226662, valid loss: 0.359327, best valid loss: 0.312407\n",
            "Epoch 0019, train loss: 0.217719, valid loss: 0.351286, best valid loss: 0.312407\n",
            "Epoch 0020, train loss: 0.215089, valid loss: 0.386109, best valid loss: 0.312407\n",
            "Early stopping, best valid loss: 0.312407\n",
            "--------------\n",
            "gru_128_6__bidirectiona_True__decoder_3\n",
            "Epoch 0000, train loss: 0.450305, valid loss: 0.422139, best valid loss: inf\n",
            "Epoch 0001, train loss: 0.426010, valid loss: 0.395225, best valid loss: inf\n",
            "Epoch 0002, train loss: 0.411034, valid loss: 0.382057, best valid loss: inf\n",
            "Epoch 0003, train loss: 0.399101, valid loss: 0.457506, best valid loss: 0.457506\n",
            "Epoch 0004, train loss: 0.397204, valid loss: 0.324683, best valid loss: 0.457506\n",
            "Epoch 0005, train loss: 0.387654, valid loss: 0.457070, best valid loss: 0.457070\n",
            "Epoch 0006, train loss: 0.345191, valid loss: 0.316251, best valid loss: 0.457070\n",
            "Epoch 0007, train loss: 0.327876, valid loss: 0.405224, best valid loss: 0.405224\n",
            "Epoch 0008, train loss: 0.297052, valid loss: 0.381365, best valid loss: 0.381365\n",
            "Epoch 0009, train loss: 0.292415, valid loss: 0.306063, best valid loss: 0.306063\n",
            "Epoch 0010, train loss: 0.298918, valid loss: 0.350173, best valid loss: 0.306063\n",
            "Epoch 0011, train loss: 0.297039, valid loss: 0.407662, best valid loss: 0.306063\n",
            "Epoch 0012, train loss: 0.245824, valid loss: 0.344641, best valid loss: 0.306063\n",
            "Epoch 0013, train loss: 0.246212, valid loss: 0.300846, best valid loss: 0.300846\n",
            "Epoch 0014, train loss: 0.226327, valid loss: 0.322461, best valid loss: 0.300846\n",
            "Epoch 0015, train loss: 0.203760, valid loss: 0.304706, best valid loss: 0.300846\n",
            "Epoch 0016, train loss: 0.202808, valid loss: 0.357457, best valid loss: 0.300846\n",
            "Epoch 0017, train loss: 0.207141, valid loss: 0.360814, best valid loss: 0.300846\n",
            "Epoch 0018, train loss: 0.178004, valid loss: 0.434233, best valid loss: 0.300846\n",
            "Epoch 0019, train loss: 0.172772, valid loss: 0.336014, best valid loss: 0.300846\n",
            "Epoch 0020, train loss: 0.164654, valid loss: 0.377598, best valid loss: 0.300846\n",
            "Epoch 0021, train loss: 0.143591, valid loss: 0.353713, best valid loss: 0.300846\n",
            "Epoch 0022, train loss: 0.144887, valid loss: 0.339852, best valid loss: 0.300846\n",
            "Epoch 0023, train loss: 0.127271, valid loss: 0.380230, best valid loss: 0.300846\n",
            "Early stopping, best valid loss: 0.300846\n",
            "--------------\n",
            "gru_64_4__bidirectiona_True__decoder_3\n",
            "Epoch 0000, train loss: 0.445072, valid loss: 0.401371, best valid loss: inf\n",
            "Epoch 0001, train loss: 0.453079, valid loss: 0.408757, best valid loss: inf\n",
            "Epoch 0002, train loss: 0.418126, valid loss: 0.495503, best valid loss: 0.495503\n",
            "Epoch 0003, train loss: 0.408958, valid loss: 0.453531, best valid loss: 0.453531\n",
            "Epoch 0004, train loss: 0.396334, valid loss: 0.362677, best valid loss: 0.453531\n",
            "Epoch 0005, train loss: 0.395752, valid loss: 0.361919, best valid loss: 0.453531\n",
            "Epoch 0006, train loss: 0.359525, valid loss: 0.406959, best valid loss: 0.406959\n",
            "Epoch 0007, train loss: 0.391759, valid loss: 0.373756, best valid loss: 0.406959\n",
            "Epoch 0008, train loss: 0.317598, valid loss: 0.331380, best valid loss: 0.331380\n",
            "Epoch 0009, train loss: 0.302647, valid loss: 0.346836, best valid loss: 0.331380\n",
            "Epoch 0010, train loss: 0.301233, valid loss: 0.304050, best valid loss: 0.304050\n",
            "Epoch 0011, train loss: 0.276702, valid loss: 0.299570, best valid loss: 0.299570\n",
            "Epoch 0012, train loss: 0.294840, valid loss: 0.337334, best valid loss: 0.299570\n",
            "Epoch 0013, train loss: 0.262404, valid loss: 0.410448, best valid loss: 0.299570\n",
            "Epoch 0014, train loss: 0.245211, valid loss: 0.340878, best valid loss: 0.299570\n",
            "Epoch 0015, train loss: 0.239002, valid loss: 0.339588, best valid loss: 0.299570\n",
            "Epoch 0016, train loss: 0.233158, valid loss: 0.298499, best valid loss: 0.298499\n",
            "Epoch 0017, train loss: 0.229895, valid loss: 0.345673, best valid loss: 0.298499\n",
            "Epoch 0018, train loss: 0.213107, valid loss: 0.309397, best valid loss: 0.298499\n",
            "Epoch 0019, train loss: 0.204250, valid loss: 0.327029, best valid loss: 0.298499\n",
            "Epoch 0020, train loss: 0.199413, valid loss: 0.350730, best valid loss: 0.298499\n",
            "Epoch 0021, train loss: 0.193514, valid loss: 0.312846, best valid loss: 0.298499\n",
            "Epoch 0022, train loss: 0.178378, valid loss: 0.330758, best valid loss: 0.298499\n",
            "Epoch 0023, train loss: 0.180998, valid loss: 0.471283, best valid loss: 0.298499\n",
            "Epoch 0024, train loss: 0.165501, valid loss: 0.349217, best valid loss: 0.298499\n",
            "Epoch 0025, train loss: 0.159430, valid loss: 0.313330, best valid loss: 0.298499\n",
            "Epoch 0026, train loss: 0.148823, valid loss: 0.382481, best valid loss: 0.298499\n",
            "Early stopping, best valid loss: 0.298499\n",
            "--------------\n",
            "gru_64_8__bidirectiona_True__decoder_3\n",
            "Epoch 0000, train loss: 0.556061, valid loss: 0.572748, best valid loss: 0.572748\n",
            "Epoch 0001, train loss: 0.423452, valid loss: 0.443021, best valid loss: 0.443021\n",
            "Epoch 0002, train loss: 0.421319, valid loss: 0.363503, best valid loss: 0.443021\n",
            "Epoch 0003, train loss: 0.414475, valid loss: 0.376293, best valid loss: 0.443021\n",
            "Epoch 0004, train loss: 0.436255, valid loss: 0.386585, best valid loss: 0.443021\n",
            "Epoch 0005, train loss: 0.377287, valid loss: 0.379009, best valid loss: 0.379009\n",
            "Epoch 0006, train loss: 0.340603, valid loss: 0.328226, best valid loss: 0.379009\n",
            "Epoch 0007, train loss: 0.321062, valid loss: 0.320820, best valid loss: 0.379009\n",
            "Epoch 0008, train loss: 0.311758, valid loss: 0.327325, best valid loss: 0.327325\n",
            "Epoch 0009, train loss: 0.335132, valid loss: 0.354789, best valid loss: 0.327325\n",
            "Epoch 0010, train loss: 0.275944, valid loss: 0.311121, best valid loss: 0.311121\n",
            "Epoch 0011, train loss: 0.283800, valid loss: 0.420411, best valid loss: 0.311121\n",
            "Epoch 0012, train loss: 0.256503, valid loss: 0.301180, best valid loss: 0.301180\n",
            "Epoch 0013, train loss: 0.243138, valid loss: 0.320797, best valid loss: 0.301180\n",
            "Epoch 0014, train loss: 0.242064, valid loss: 0.366762, best valid loss: 0.301180\n",
            "Epoch 0015, train loss: 0.233520, valid loss: 0.330279, best valid loss: 0.301180\n",
            "Epoch 0016, train loss: 0.239893, valid loss: 0.328368, best valid loss: 0.301180\n",
            "Epoch 0017, train loss: 0.218270, valid loss: 0.311899, best valid loss: 0.301180\n",
            "Epoch 0018, train loss: 0.234227, valid loss: 0.470602, best valid loss: 0.301180\n",
            "Epoch 0019, train loss: 0.188344, valid loss: 0.303205, best valid loss: 0.301180\n",
            "Epoch 0020, train loss: 0.183450, valid loss: 0.333264, best valid loss: 0.301180\n",
            "Epoch 0021, train loss: 0.170418, valid loss: 0.340583, best valid loss: 0.301180\n",
            "Epoch 0022, train loss: 0.187185, valid loss: 0.344737, best valid loss: 0.301180\n",
            "Early stopping, best valid loss: 0.301180\n",
            "--------------\n",
            "gru_64_6__bidirectiona_False__decoder_3\n",
            "Epoch 0000, train loss: 0.789172, valid loss: 0.784560, best valid loss: inf\n",
            "Epoch 0001, train loss: 0.432647, valid loss: 0.422811, best valid loss: inf\n",
            "Epoch 0002, train loss: 0.420887, valid loss: 0.498768, best valid loss: 0.498768\n",
            "Epoch 0003, train loss: 0.414743, valid loss: 0.364185, best valid loss: 0.498768\n",
            "Epoch 0004, train loss: 0.403882, valid loss: 0.414459, best valid loss: 0.414459\n",
            "Epoch 0005, train loss: 0.390292, valid loss: 0.353569, best valid loss: 0.414459\n",
            "Epoch 0006, train loss: 0.363844, valid loss: 0.323465, best valid loss: 0.414459\n",
            "Epoch 0007, train loss: 0.350103, valid loss: 0.349479, best valid loss: 0.414459\n",
            "Epoch 0008, train loss: 0.329865, valid loss: 0.377029, best valid loss: 0.377029\n",
            "Epoch 0009, train loss: 0.316631, valid loss: 0.368889, best valid loss: 0.368889\n",
            "Epoch 0010, train loss: 0.337319, valid loss: 0.342467, best valid loss: 0.342467\n",
            "Epoch 0011, train loss: 0.299808, valid loss: 0.344233, best valid loss: 0.342467\n",
            "Epoch 0012, train loss: 0.293313, valid loss: 0.311589, best valid loss: 0.311589\n",
            "Epoch 0013, train loss: 0.302793, valid loss: 0.334076, best valid loss: 0.311589\n",
            "Epoch 0014, train loss: 0.279958, valid loss: 0.319235, best valid loss: 0.311589\n",
            "Epoch 0015, train loss: 0.273545, valid loss: 0.320671, best valid loss: 0.311589\n",
            "Epoch 0016, train loss: 0.268983, valid loss: 0.305389, best valid loss: 0.305389\n",
            "Epoch 0017, train loss: 0.264467, valid loss: 0.319734, best valid loss: 0.305389\n",
            "Epoch 0018, train loss: 0.280599, valid loss: 0.295866, best valid loss: 0.295866\n",
            "Epoch 0019, train loss: 0.279652, valid loss: 0.328065, best valid loss: 0.295866\n",
            "Epoch 0020, train loss: 0.261302, valid loss: 0.324586, best valid loss: 0.295866\n",
            "Epoch 0021, train loss: 0.238788, valid loss: 0.361580, best valid loss: 0.295866\n",
            "Epoch 0022, train loss: 0.228657, valid loss: 0.340389, best valid loss: 0.295866\n",
            "Epoch 0023, train loss: 0.223110, valid loss: 0.337166, best valid loss: 0.295866\n",
            "Epoch 0024, train loss: 0.228625, valid loss: 0.370830, best valid loss: 0.295866\n",
            "Epoch 0025, train loss: 0.214911, valid loss: 0.332959, best valid loss: 0.295866\n",
            "Epoch 0026, train loss: 0.235175, valid loss: 0.357602, best valid loss: 0.295866\n",
            "Epoch 0027, train loss: 0.201465, valid loss: 0.325106, best valid loss: 0.295866\n",
            "Epoch 0028, train loss: 0.196484, valid loss: 0.329611, best valid loss: 0.295866\n",
            "Early stopping, best valid loss: 0.295866\n",
            "--------------\n",
            "gru_64_6__bidirectiona_True__decoder_2\n",
            "Epoch 0000, train loss: 0.575563, valid loss: 0.460245, best valid loss: inf\n",
            "Epoch 0001, train loss: 0.424995, valid loss: 0.352634, best valid loss: inf\n",
            "Epoch 0002, train loss: 0.422857, valid loss: 0.416253, best valid loss: inf\n",
            "Epoch 0003, train loss: 0.415287, valid loss: 0.420683, best valid loss: 0.420683\n",
            "Epoch 0004, train loss: 0.406473, valid loss: 0.380723, best valid loss: 0.420683\n",
            "Epoch 0005, train loss: 0.399811, valid loss: 0.398830, best valid loss: 0.420683\n",
            "Epoch 0006, train loss: 0.387844, valid loss: 0.401270, best valid loss: 0.401270\n",
            "Epoch 0007, train loss: 0.362599, valid loss: 0.319287, best valid loss: 0.401270\n",
            "Epoch 0008, train loss: 0.326881, valid loss: 0.299875, best valid loss: 0.401270\n",
            "Epoch 0009, train loss: 0.307938, valid loss: 0.368863, best valid loss: 0.368863\n",
            "Epoch 0010, train loss: 0.293738, valid loss: 0.336112, best valid loss: 0.336112\n",
            "Epoch 0011, train loss: 0.287543, valid loss: 0.289827, best valid loss: 0.289827\n",
            "Epoch 0012, train loss: 0.276884, valid loss: 0.352215, best valid loss: 0.289827\n",
            "Epoch 0013, train loss: 0.296263, valid loss: 0.436450, best valid loss: 0.289827\n",
            "Epoch 0014, train loss: 0.275930, valid loss: 0.373474, best valid loss: 0.289827\n",
            "Epoch 0015, train loss: 0.243612, valid loss: 0.298650, best valid loss: 0.289827\n",
            "Epoch 0016, train loss: 0.232176, valid loss: 0.311462, best valid loss: 0.289827\n",
            "Epoch 0017, train loss: 0.226563, valid loss: 0.356029, best valid loss: 0.289827\n",
            "Epoch 0018, train loss: 0.217503, valid loss: 0.339872, best valid loss: 0.289827\n",
            "Epoch 0019, train loss: 0.213578, valid loss: 0.353794, best valid loss: 0.289827\n",
            "Epoch 0020, train loss: 0.197522, valid loss: 0.418965, best valid loss: 0.289827\n",
            "Epoch 0021, train loss: 0.185699, valid loss: 0.329470, best valid loss: 0.289827\n",
            "Early stopping, best valid loss: 0.289827\n",
            "--------------\n",
            "gru_64_6__bidirectiona_True__decoder_4\n",
            "Epoch 0000, train loss: 0.819062, valid loss: 0.944590, best valid loss: 0.944590\n",
            "Epoch 0001, train loss: 0.441804, valid loss: 0.356974, best valid loss: 0.944590\n",
            "Epoch 0002, train loss: 0.423881, valid loss: 0.390668, best valid loss: 0.944590\n",
            "Epoch 0003, train loss: 0.416766, valid loss: 0.385705, best valid loss: 0.944590\n",
            "Epoch 0004, train loss: 0.411706, valid loss: 0.427284, best valid loss: 0.427284\n",
            "Epoch 0005, train loss: 0.399432, valid loss: 0.403982, best valid loss: 0.403982\n",
            "Epoch 0006, train loss: 0.373655, valid loss: 0.366817, best valid loss: 0.403982\n",
            "Epoch 0007, train loss: 0.348559, valid loss: 0.335614, best valid loss: 0.403982\n",
            "Epoch 0008, train loss: 0.325680, valid loss: 0.350833, best valid loss: 0.350833\n",
            "Epoch 0009, train loss: 0.300113, valid loss: 0.309031, best valid loss: 0.309031\n",
            "Epoch 0010, train loss: 0.286285, valid loss: 0.329247, best valid loss: 0.309031\n",
            "Epoch 0011, train loss: 0.280605, valid loss: 0.281590, best valid loss: 0.281590\n",
            "Epoch 0012, train loss: 0.274004, valid loss: 0.336776, best valid loss: 0.281590\n",
            "Epoch 0013, train loss: 0.269247, valid loss: 0.307692, best valid loss: 0.281590\n",
            "Epoch 0014, train loss: 0.269556, valid loss: 0.355824, best valid loss: 0.281590\n",
            "Epoch 0015, train loss: 0.245212, valid loss: 0.307941, best valid loss: 0.281590\n",
            "Epoch 0016, train loss: 0.258183, valid loss: 0.325393, best valid loss: 0.281590\n",
            "Epoch 0017, train loss: 0.281674, valid loss: 0.369541, best valid loss: 0.281590\n",
            "Epoch 0018, train loss: 0.220729, valid loss: 0.360664, best valid loss: 0.281590\n",
            "Epoch 0019, train loss: 0.217592, valid loss: 0.353148, best valid loss: 0.281590\n",
            "Epoch 0020, train loss: 0.226578, valid loss: 0.306730, best valid loss: 0.281590\n",
            "Epoch 0021, train loss: 0.204426, valid loss: 0.356330, best valid loss: 0.281590\n",
            "Early stopping, best valid loss: 0.281590\n",
            "--------------\n"
          ]
        }
      ],
      "source": [
        "for name, params in model_versions.items():\n",
        "    print(name)\n",
        "    gru_grader = GRUGrader(gensim_emb_weights=gensim_emb_weights, **params)\n",
        "    best_model = train_grader(dl_train, dl_valid, \n",
        "        model=gru_grader,\n",
        "        model_name=name,\n",
        "        save_path=\"drive/MyDrive/data/cs7641/rnn_models\",\n",
        "        loss_func=torch.nn.MSELoss(),\n",
        "        opt=torch.optim.Adam(gru_grader.parameters(), lr=0.001),\n",
        "        epochs=30)\n",
        "    print(\"--------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kZgGo0kle4xG"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_versions2 = {\n",
        "    # # Experiment with gru depth\n",
        "    \"gru_64_12__bidirectiona_True__decoder_3\":{\n",
        "        \"gru_size\":64,\n",
        "        \"gru_num_layer\":12,\n",
        "        \"bidirectional\":True,\n",
        "        \"decoder_depth\":3,\n",
        "        \"decoder_size\":[256, 512, 64]\n",
        "    },\n",
        "    \"gru_64_16__bidirectiona_True__decoder_3\":{\n",
        "        \"gru_size\":64,\n",
        "        \"gru_num_layer\":16,\n",
        "        \"bidirectional\":True,\n",
        "        \"decoder_depth\":3,\n",
        "        \"decoder_size\":[256, 512, 64]\n",
        "    },\n",
        "    # Experiment with decoder size\n",
        "    \"gru_64_12__bidirectiona_True__decoder_3_lite\":{\n",
        "        \"gru_size\":64,\n",
        "        \"gru_num_layer\":12,\n",
        "        \"bidirectional\":True,\n",
        "        \"decoder_depth\":3,\n",
        "        \"decoder_size\":[128, 256, 64]\n",
        "    },\n",
        "    \"gru_64_16__bidirectiona_True__decoder_3_lite\":{\n",
        "        \"gru_size\":64,\n",
        "        \"gru_num_layer\":16,\n",
        "        \"bidirectional\":True,\n",
        "        \"decoder_depth\":3,\n",
        "        \"decoder_size\":[128, 256, 64]\n",
        "    },\n",
        "    # Experiment with decoder depth\n",
        "    \"gru_64_12__bidirectiona_True__decoder_4_lite\":{\n",
        "        \"gru_size\":64,\n",
        "        \"gru_num_layer\":12,\n",
        "        \"bidirectional\":True,\n",
        "        \"decoder_depth\":4,\n",
        "        \"decoder_size\":[128, 256, 128, 64]\n",
        "    },\n",
        "    \"gru_64_16__bidirectiona_True__decoder_4_lite\":{\n",
        "        \"gru_size\":64,\n",
        "        \"gru_num_layer\":16,\n",
        "        \"bidirectional\":True,\n",
        "        \"decoder_depth\":4,\n",
        "        \"decoder_size\":[128, 256, 128, 64]\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_S5EjzCBe4xG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edbf1b8b-0e97-4144-dbe1-83351643d6c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gru_64_12__bidirectiona_True__decoder_3\n",
            "Epoch 0000, train loss: 0.654184, valid loss: 0.613599, best valid loss: inf\n",
            "Epoch 0001, train loss: 0.432877, valid loss: 0.389186, best valid loss: inf\n",
            "Epoch 0002, train loss: 0.421245, valid loss: 0.367764, best valid loss: inf\n",
            "Epoch 0003, train loss: 0.416117, valid loss: 0.356951, best valid loss: inf\n",
            "Epoch 0004, train loss: 0.406878, valid loss: 0.413490, best valid loss: 0.413490\n",
            "Epoch 0005, train loss: 0.394382, valid loss: 0.337150, best valid loss: 0.413490\n",
            "Epoch 0006, train loss: 0.368025, valid loss: 0.343445, best valid loss: 0.413490\n",
            "Epoch 0007, train loss: 0.333652, valid loss: 0.386843, best valid loss: 0.386843\n",
            "Epoch 0008, train loss: 0.328986, valid loss: 0.328809, best valid loss: 0.386843\n",
            "Epoch 0009, train loss: 0.292586, valid loss: 0.278185, best valid loss: 0.386843\n",
            "Epoch 0010, train loss: 0.293106, valid loss: 0.398448, best valid loss: 0.386843\n",
            "Epoch 0011, train loss: 0.280748, valid loss: 0.325965, best valid loss: 0.325965\n",
            "Epoch 0012, train loss: 0.302170, valid loss: 0.371294, best valid loss: 0.325965\n",
            "Epoch 0013, train loss: 0.259480, valid loss: 0.292613, best valid loss: 0.292613\n",
            "Epoch 0014, train loss: 0.245076, valid loss: 0.310835, best valid loss: 0.292613\n",
            "Epoch 0015, train loss: 0.283169, valid loss: 0.390719, best valid loss: 0.292613\n",
            "Epoch 0016, train loss: 0.230557, valid loss: 0.300861, best valid loss: 0.292613\n",
            "Epoch 0017, train loss: 0.223732, valid loss: 0.319644, best valid loss: 0.292613\n",
            "Epoch 0018, train loss: 0.213020, valid loss: 0.337338, best valid loss: 0.292613\n",
            "Epoch 0019, train loss: 0.196753, valid loss: 0.290767, best valid loss: 0.290767\n",
            "Epoch 0020, train loss: 0.224513, valid loss: 0.323928, best valid loss: 0.290767\n",
            "Epoch 0021, train loss: 0.199245, valid loss: 0.313612, best valid loss: 0.290767\n",
            "Epoch 0022, train loss: 0.179038, valid loss: 0.315277, best valid loss: 0.290767\n",
            "Epoch 0023, train loss: 0.165570, valid loss: 0.418965, best valid loss: 0.290767\n",
            "Epoch 0024, train loss: 0.157275, valid loss: 0.356714, best valid loss: 0.290767\n",
            "Epoch 0025, train loss: 0.158844, valid loss: 0.342130, best valid loss: 0.290767\n",
            "Epoch 0026, train loss: 0.148833, valid loss: 0.367256, best valid loss: 0.290767\n",
            "Epoch 0027, train loss: 0.137502, valid loss: 0.357422, best valid loss: 0.290767\n",
            "Epoch 0028, train loss: 0.135017, valid loss: 0.356788, best valid loss: 0.290767\n",
            "Epoch 0029, train loss: 0.127780, valid loss: 0.502672, best valid loss: 0.290767\n",
            "Early stopping, best valid loss: 0.290767\n",
            "--------------\n",
            "gru_64_16__bidirectiona_True__decoder_3\n",
            "Epoch 0000, train loss: 0.501756, valid loss: 0.477813, best valid loss: inf\n",
            "Epoch 0001, train loss: 0.427504, valid loss: 0.375993, best valid loss: inf\n",
            "Epoch 0002, train loss: 0.422395, valid loss: 0.422655, best valid loss: 0.422655\n",
            "Epoch 0003, train loss: 0.414124, valid loss: 0.357872, best valid loss: 0.422655\n",
            "Epoch 0004, train loss: 0.404126, valid loss: 0.332881, best valid loss: 0.422655\n",
            "Epoch 0005, train loss: 0.384477, valid loss: 0.343346, best valid loss: 0.422655\n",
            "Epoch 0006, train loss: 0.360625, valid loss: 0.337266, best valid loss: 0.422655\n",
            "Epoch 0007, train loss: 0.324187, valid loss: 0.325263, best valid loss: 0.325263\n",
            "Epoch 0008, train loss: 0.307296, valid loss: 0.324747, best valid loss: 0.324747\n",
            "Epoch 0009, train loss: 0.303841, valid loss: 0.337808, best valid loss: 0.324747\n",
            "Epoch 0010, train loss: 0.275710, valid loss: 0.328977, best valid loss: 0.324747\n",
            "Epoch 0011, train loss: 0.280362, valid loss: 0.319690, best valid loss: 0.319690\n",
            "Epoch 0012, train loss: 0.277579, valid loss: 0.397427, best valid loss: 0.319690\n",
            "Epoch 0013, train loss: 0.245852, valid loss: 0.340954, best valid loss: 0.319690\n",
            "Epoch 0014, train loss: 0.232609, valid loss: 0.307871, best valid loss: 0.307871\n",
            "Epoch 0015, train loss: 0.230031, valid loss: 0.376415, best valid loss: 0.307871\n",
            "Epoch 0016, train loss: 0.257352, valid loss: 0.355979, best valid loss: 0.307871\n",
            "Epoch 0017, train loss: 0.202294, valid loss: 0.340525, best valid loss: 0.307871\n",
            "Epoch 0018, train loss: 0.190874, valid loss: 0.399763, best valid loss: 0.307871\n",
            "Epoch 0019, train loss: 0.184885, valid loss: 0.301681, best valid loss: 0.301681\n",
            "Epoch 0020, train loss: 0.176081, valid loss: 0.339720, best valid loss: 0.301681\n",
            "Epoch 0021, train loss: 0.173211, valid loss: 0.345975, best valid loss: 0.301681\n",
            "Epoch 0022, train loss: 0.159656, valid loss: 0.363291, best valid loss: 0.301681\n",
            "Epoch 0023, train loss: 0.166442, valid loss: 0.373065, best valid loss: 0.301681\n",
            "Epoch 0024, train loss: 0.144262, valid loss: 0.384321, best valid loss: 0.301681\n",
            "Epoch 0025, train loss: 0.135249, valid loss: 0.342581, best valid loss: 0.301681\n",
            "Epoch 0026, train loss: 0.136630, valid loss: 0.458027, best valid loss: 0.301681\n",
            "Epoch 0027, train loss: 0.161109, valid loss: 0.440417, best valid loss: 0.301681\n",
            "Epoch 0028, train loss: 0.144612, valid loss: 0.539701, best valid loss: 0.301681\n",
            "Epoch 0029, train loss: 0.148208, valid loss: 0.360421, best valid loss: 0.301681\n",
            "Early stopping, best valid loss: 0.301681\n",
            "--------------\n",
            "gru_64_12__bidirectiona_True__decoder_3_lite\n",
            "Epoch 0000, train loss: 0.750620, valid loss: 0.661225, best valid loss: inf\n",
            "Epoch 0001, train loss: 0.432896, valid loss: 0.419167, best valid loss: inf\n",
            "Epoch 0002, train loss: 0.425955, valid loss: 0.375623, best valid loss: inf\n",
            "Epoch 0003, train loss: 0.417732, valid loss: 0.461833, best valid loss: 0.461833\n",
            "Epoch 0004, train loss: 0.419710, valid loss: 0.366318, best valid loss: 0.461833\n",
            "Epoch 0005, train loss: 0.425264, valid loss: 0.348162, best valid loss: 0.461833\n",
            "Epoch 0006, train loss: 0.418540, valid loss: 0.400429, best valid loss: 0.461833\n",
            "Epoch 0007, train loss: 0.379859, valid loss: 0.480348, best valid loss: 0.461833\n",
            "Epoch 0008, train loss: 0.359717, valid loss: 0.339484, best valid loss: 0.461833\n",
            "Epoch 0009, train loss: 0.329135, valid loss: 0.425533, best valid loss: 0.425533\n",
            "Epoch 0010, train loss: 0.319545, valid loss: 0.349717, best valid loss: 0.349717\n",
            "Epoch 0011, train loss: 0.298702, valid loss: 0.291836, best valid loss: 0.349717\n",
            "Epoch 0012, train loss: 0.296019, valid loss: 0.344719, best valid loss: 0.344719\n",
            "Epoch 0013, train loss: 0.303724, valid loss: 0.342365, best valid loss: 0.342365\n",
            "Epoch 0014, train loss: 0.274277, valid loss: 0.287748, best valid loss: 0.287748\n",
            "Epoch 0015, train loss: 0.255212, valid loss: 0.321316, best valid loss: 0.287748\n",
            "Epoch 0016, train loss: 0.241378, valid loss: 0.302448, best valid loss: 0.287748\n",
            "Epoch 0017, train loss: 0.233684, valid loss: 0.308631, best valid loss: 0.287748\n",
            "Epoch 0018, train loss: 0.227526, valid loss: 0.300300, best valid loss: 0.287748\n",
            "Epoch 0019, train loss: 0.220885, valid loss: 0.314336, best valid loss: 0.287748\n",
            "Epoch 0020, train loss: 0.204223, valid loss: 0.348802, best valid loss: 0.287748\n",
            "Epoch 0021, train loss: 0.203753, valid loss: 0.303924, best valid loss: 0.287748\n",
            "Epoch 0022, train loss: 0.186816, valid loss: 0.384410, best valid loss: 0.287748\n",
            "Epoch 0023, train loss: 0.212180, valid loss: 0.363709, best valid loss: 0.287748\n",
            "Epoch 0024, train loss: 0.185643, valid loss: 0.362539, best valid loss: 0.287748\n",
            "Early stopping, best valid loss: 0.287748\n",
            "--------------\n",
            "gru_64_16__bidirectiona_True__decoder_3_lite\n",
            "Epoch 0000, train loss: 0.430397, valid loss: 0.458114, best valid loss: 0.458114\n",
            "Epoch 0001, train loss: 0.443350, valid loss: 0.409474, best valid loss: 0.458114\n",
            "Epoch 0002, train loss: 0.424375, valid loss: 0.398923, best valid loss: 0.458114\n",
            "Epoch 0003, train loss: 0.429340, valid loss: 0.448647, best valid loss: 0.448647\n",
            "Epoch 0004, train loss: 0.416532, valid loss: 0.436546, best valid loss: 0.436546\n",
            "Epoch 0005, train loss: 0.410264, valid loss: 0.343862, best valid loss: 0.436546\n",
            "Epoch 0006, train loss: 0.439758, valid loss: 0.368523, best valid loss: 0.436546\n",
            "Epoch 0007, train loss: 0.394558, valid loss: 0.423678, best valid loss: 0.423678\n",
            "Epoch 0008, train loss: 0.372841, valid loss: 0.353775, best valid loss: 0.423678\n",
            "Epoch 0009, train loss: 0.343709, valid loss: 0.362679, best valid loss: 0.362679\n",
            "Epoch 0010, train loss: 0.316274, valid loss: 0.297851, best valid loss: 0.362679\n",
            "Epoch 0011, train loss: 0.296096, valid loss: 0.325988, best valid loss: 0.325988\n",
            "Epoch 0012, train loss: 0.291129, valid loss: 0.293098, best valid loss: 0.293098\n",
            "Epoch 0013, train loss: 0.271241, valid loss: 0.386265, best valid loss: 0.293098\n",
            "Epoch 0014, train loss: 0.283413, valid loss: 0.403215, best valid loss: 0.293098\n",
            "Epoch 0015, train loss: 0.252076, valid loss: 0.290400, best valid loss: 0.290400\n",
            "Epoch 0016, train loss: 0.259353, valid loss: 0.298241, best valid loss: 0.290400\n",
            "Epoch 0017, train loss: 0.227944, valid loss: 0.308923, best valid loss: 0.290400\n",
            "Epoch 0018, train loss: 0.228941, valid loss: 0.354070, best valid loss: 0.290400\n",
            "Epoch 0019, train loss: 0.245921, valid loss: 0.323758, best valid loss: 0.290400\n",
            "Epoch 0020, train loss: 0.201357, valid loss: 0.379183, best valid loss: 0.290400\n",
            "Epoch 0021, train loss: 0.198230, valid loss: 0.355401, best valid loss: 0.290400\n",
            "Epoch 0022, train loss: 0.193503, valid loss: 0.362231, best valid loss: 0.290400\n",
            "Epoch 0023, train loss: 0.175763, valid loss: 0.355738, best valid loss: 0.290400\n",
            "Epoch 0024, train loss: 0.163450, valid loss: 0.437557, best valid loss: 0.290400\n",
            "Epoch 0025, train loss: 0.160996, valid loss: 0.321252, best valid loss: 0.290400\n",
            "Early stopping, best valid loss: 0.290400\n",
            "--------------\n",
            "gru_64_12__bidirectiona_True__decoder_4_lite\n",
            "Epoch 0000, train loss: 0.441262, valid loss: 0.418966, best valid loss: inf\n",
            "Epoch 0001, train loss: 0.432748, valid loss: 0.444637, best valid loss: 0.444637\n",
            "Epoch 0002, train loss: 0.436386, valid loss: 0.422531, best valid loss: 0.444637\n",
            "Epoch 0003, train loss: 0.429472, valid loss: 0.376293, best valid loss: 0.444637\n",
            "Epoch 0004, train loss: 0.409167, valid loss: 0.485163, best valid loss: 0.444637\n",
            "Epoch 0005, train loss: 0.393286, valid loss: 0.351630, best valid loss: 0.444637\n",
            "Epoch 0006, train loss: 0.361932, valid loss: 0.343365, best valid loss: 0.444637\n",
            "Epoch 0007, train loss: 0.329367, valid loss: 0.329793, best valid loss: 0.329793\n",
            "Epoch 0008, train loss: 0.339282, valid loss: 0.371224, best valid loss: 0.329793\n",
            "Epoch 0009, train loss: 0.306098, valid loss: 0.326652, best valid loss: 0.326652\n",
            "Epoch 0010, train loss: 0.296244, valid loss: 0.296251, best valid loss: 0.296251\n",
            "Epoch 0011, train loss: 0.319087, valid loss: 0.382517, best valid loss: 0.296251\n",
            "Epoch 0012, train loss: 0.292109, valid loss: 0.297470, best valid loss: 0.296251\n",
            "Epoch 0013, train loss: 0.279851, valid loss: 0.315826, best valid loss: 0.296251\n",
            "Epoch 0014, train loss: 0.255412, valid loss: 0.276522, best valid loss: 0.276522\n",
            "Epoch 0015, train loss: 0.241835, valid loss: 0.282371, best valid loss: 0.276522\n",
            "Epoch 0016, train loss: 0.231385, valid loss: 0.289502, best valid loss: 0.276522\n",
            "Epoch 0017, train loss: 0.227580, valid loss: 0.302406, best valid loss: 0.276522\n",
            "Epoch 0018, train loss: 0.210144, valid loss: 0.302395, best valid loss: 0.276522\n",
            "Epoch 0019, train loss: 0.206116, valid loss: 0.354634, best valid loss: 0.276522\n",
            "Epoch 0020, train loss: 0.191430, valid loss: 0.320824, best valid loss: 0.276522\n",
            "Epoch 0021, train loss: 0.192635, valid loss: 0.295771, best valid loss: 0.276522\n",
            "Epoch 0022, train loss: 0.200272, valid loss: 0.393625, best valid loss: 0.276522\n",
            "Epoch 0023, train loss: 0.168565, valid loss: 0.357689, best valid loss: 0.276522\n",
            "Epoch 0024, train loss: 0.168737, valid loss: 0.341160, best valid loss: 0.276522\n",
            "Early stopping, best valid loss: 0.276522\n",
            "--------------\n",
            "gru_64_16__bidirectiona_True__decoder_4_lite\n",
            "Epoch 0000, train loss: 0.435769, valid loss: 0.430152, best valid loss: inf\n",
            "Epoch 0001, train loss: 0.509900, valid loss: 0.501303, best valid loss: inf\n",
            "Epoch 0002, train loss: 0.433308, valid loss: 0.390970, best valid loss: inf\n",
            "Epoch 0003, train loss: 0.439281, valid loss: 0.343306, best valid loss: inf\n",
            "Epoch 0004, train loss: 0.413062, valid loss: 0.414450, best valid loss: 0.414450\n",
            "Epoch 0005, train loss: 0.402967, valid loss: 0.419096, best valid loss: 0.414450\n",
            "Epoch 0006, train loss: 0.387661, valid loss: 0.375436, best valid loss: 0.414450\n",
            "Epoch 0007, train loss: 0.345589, valid loss: 0.364760, best valid loss: 0.364760\n",
            "Epoch 0008, train loss: 0.340127, valid loss: 0.382593, best valid loss: 0.364760\n",
            "Epoch 0009, train loss: 0.308647, valid loss: 0.326112, best valid loss: 0.326112\n",
            "Epoch 0010, train loss: 0.297645, valid loss: 0.297316, best valid loss: 0.326112\n",
            "Epoch 0011, train loss: 0.316173, valid loss: 0.297722, best valid loss: 0.326112\n",
            "Epoch 0012, train loss: 0.273697, valid loss: 0.286794, best valid loss: 0.286794\n",
            "Epoch 0013, train loss: 0.292631, valid loss: 0.302467, best valid loss: 0.286794\n",
            "Epoch 0014, train loss: 0.255330, valid loss: 0.381848, best valid loss: 0.286794\n",
            "Epoch 0015, train loss: 0.258197, valid loss: 0.292492, best valid loss: 0.286794\n",
            "Epoch 0016, train loss: 0.233987, valid loss: 0.297089, best valid loss: 0.286794\n",
            "Epoch 0017, train loss: 0.225579, valid loss: 0.331839, best valid loss: 0.286794\n",
            "Epoch 0018, train loss: 0.233980, valid loss: 0.322852, best valid loss: 0.286794\n",
            "Epoch 0019, train loss: 0.208510, valid loss: 0.305676, best valid loss: 0.286794\n",
            "Epoch 0020, train loss: 0.217270, valid loss: 0.370401, best valid loss: 0.286794\n",
            "Epoch 0021, train loss: 0.194218, valid loss: 0.355414, best valid loss: 0.286794\n",
            "Epoch 0022, train loss: 0.171532, valid loss: 0.349884, best valid loss: 0.286794\n",
            "Early stopping, best valid loss: 0.286794\n",
            "--------------\n"
          ]
        }
      ],
      "source": [
        "for name, params in model_versions2.items():\n",
        "    print(name)\n",
        "    gru_grader = GRUGrader(gensim_emb_weights=gensim_emb_weights, **params)\n",
        "    best_model = train_grader(dl_train, dl_valid, \n",
        "        model=gru_grader,\n",
        "        model_name=name,\n",
        "        save_path=\"drive/MyDrive/data/cs7641/rnn_models\",\n",
        "        loss_func=torch.nn.MSELoss(),\n",
        "        opt=torch.optim.Adam(gru_grader.parameters(), lr=0.001),\n",
        "        epochs=30)\n",
        "    print(\"--------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EYwhBNfJe4xG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.4 ('cs7641project')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "0347152bab7395cf6d4d53b744fcb402ec7f725dba3cd9225265b4a9dcbbfed9"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}