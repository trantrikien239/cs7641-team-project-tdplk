{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kientran/miniconda3/envs/cs7641project/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.downloader\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import EssayDataset, essay_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "      <th>word_token_nltk</th>\n",
       "      <th>sent_token</th>\n",
       "      <th>word_token_manual</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemm_text</th>\n",
       "      <th>freq_dist</th>\n",
       "      <th>most_common_words</th>\n",
       "      <th>distinct_words_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>['I', 'think', 'that', 'students', 'would', 'b...</td>\n",
       "      <td>['I think that students would benefit from lea...</td>\n",
       "      <td>['I', 'think', 'that', 'students', 'would', 'b...</td>\n",
       "      <td>['think', 'students', 'would', 'benefit', 'lea...</td>\n",
       "      <td>['think', 'student', 'would', 'benefit', 'lear...</td>\n",
       "      <td>&lt;FreqDist with 81 samples and 129 outcomes&gt;</td>\n",
       "      <td>[('student', 5), ('class', 5), ('go', 5)]</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>When a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>['When', 'a', 'problem', 'is', 'a', 'change', ...</td>\n",
       "      <td>['When a problem is a change you have to let i...</td>\n",
       "      <td>['When', 'a', 'problem', 'is', 'a', 'change', ...</td>\n",
       "      <td>['problem', 'change', 'let', 'best', 'matter',...</td>\n",
       "      <td>['problem', 'change', 'let', 'best', 'matter',...</td>\n",
       "      <td>&lt;FreqDist with 80 samples and 215 outcomes&gt;</td>\n",
       "      <td>[('change', 16), ('different', 12), ('problem'...</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>['Dear', ',', 'Principal', 'If', 'u', 'change'...</td>\n",
       "      <td>['Dear, Principal\\n\\nIf u change the school po...</td>\n",
       "      <td>['Dear,', 'Principal\\n\\nIf', 'u', 'change', 't...</td>\n",
       "      <td>['dear', 'principal', 'u', 'change', 'school',...</td>\n",
       "      <td>['dear', 'principal', 'u', 'change', 'school',...</td>\n",
       "      <td>&lt;FreqDist with 58 samples and 133 outcomes&gt;</td>\n",
       "      <td>[('school', 9), ('average', 9), ('sport', 8)]</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>The best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['The', 'best', 'time', 'in', 'life', 'is', 'w...</td>\n",
       "      <td>['The best time in life is when you become you...</td>\n",
       "      <td>['The', 'best', 'time', 'in', 'life', 'is', 'w...</td>\n",
       "      <td>['best', 'time', 'life', 'become', 'agree', 'g...</td>\n",
       "      <td>['best', 'time', 'life', 'become', 'agree', 'g...</td>\n",
       "      <td>&lt;FreqDist with 132 samples and 282 outcomes&gt;</td>\n",
       "      <td>[('make', 16), ('choice', 10), ('others', 8)]</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>Small act of kindness can impact in other peop...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>['Small', 'act', 'of', 'kindness', 'can', 'imp...</td>\n",
       "      <td>['Small act of kindness can impact in other pe...</td>\n",
       "      <td>['Small', 'act', 'of', 'kindness', 'can', 'imp...</td>\n",
       "      <td>['small', 'act', 'kindness', 'impact', 'people...</td>\n",
       "      <td>['small', 'act', 'kindness', 'impact', 'people...</td>\n",
       "      <td>&lt;FreqDist with 67 samples and 112 outcomes&gt;</td>\n",
       "      <td>[('people', 6), ('person', 6), ('act', 5)]</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
       "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n",
       "3  003885A45F42  The best time in life is when you become yours...       4.5   \n",
       "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions  \\\n",
       "0     3.5         3.0          3.0      4.0          3.0   \n",
       "1     2.5         3.0          2.0      2.0          2.5   \n",
       "2     3.5         3.0          3.0      3.0          2.5   \n",
       "3     4.5         4.5          4.5      4.0          5.0   \n",
       "4     3.0         3.0          3.0      2.5          2.5   \n",
       "\n",
       "                                     word_token_nltk  \\\n",
       "0  ['I', 'think', 'that', 'students', 'would', 'b...   \n",
       "1  ['When', 'a', 'problem', 'is', 'a', 'change', ...   \n",
       "2  ['Dear', ',', 'Principal', 'If', 'u', 'change'...   \n",
       "3  ['The', 'best', 'time', 'in', 'life', 'is', 'w...   \n",
       "4  ['Small', 'act', 'of', 'kindness', 'can', 'imp...   \n",
       "\n",
       "                                          sent_token  \\\n",
       "0  ['I think that students would benefit from lea...   \n",
       "1  ['When a problem is a change you have to let i...   \n",
       "2  ['Dear, Principal\\n\\nIf u change the school po...   \n",
       "3  ['The best time in life is when you become you...   \n",
       "4  ['Small act of kindness can impact in other pe...   \n",
       "\n",
       "                                   word_token_manual  \\\n",
       "0  ['I', 'think', 'that', 'students', 'would', 'b...   \n",
       "1  ['When', 'a', 'problem', 'is', 'a', 'change', ...   \n",
       "2  ['Dear,', 'Principal\\n\\nIf', 'u', 'change', 't...   \n",
       "3  ['The', 'best', 'time', 'in', 'life', 'is', 'w...   \n",
       "4  ['Small', 'act', 'of', 'kindness', 'can', 'imp...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  ['think', 'students', 'would', 'benefit', 'lea...   \n",
       "1  ['problem', 'change', 'let', 'best', 'matter',...   \n",
       "2  ['dear', 'principal', 'u', 'change', 'school',...   \n",
       "3  ['best', 'time', 'life', 'become', 'agree', 'g...   \n",
       "4  ['small', 'act', 'kindness', 'impact', 'people...   \n",
       "\n",
       "                                           lemm_text  \\\n",
       "0  ['think', 'student', 'would', 'benefit', 'lear...   \n",
       "1  ['problem', 'change', 'let', 'best', 'matter',...   \n",
       "2  ['dear', 'principal', 'u', 'change', 'school',...   \n",
       "3  ['best', 'time', 'life', 'become', 'agree', 'g...   \n",
       "4  ['small', 'act', 'kindness', 'impact', 'people...   \n",
       "\n",
       "                                      freq_dist  \\\n",
       "0   <FreqDist with 81 samples and 129 outcomes>   \n",
       "1   <FreqDist with 80 samples and 215 outcomes>   \n",
       "2   <FreqDist with 58 samples and 133 outcomes>   \n",
       "3  <FreqDist with 132 samples and 282 outcomes>   \n",
       "4   <FreqDist with 67 samples and 112 outcomes>   \n",
       "\n",
       "                                   most_common_words  distinct_words_cnt  \n",
       "0          [('student', 5), ('class', 5), ('go', 5)]                  81  \n",
       "1  [('change', 16), ('different', 12), ('problem'...                  80  \n",
       "2      [('school', 9), ('average', 9), ('sport', 8)]                  58  \n",
       "3      [('make', 16), ('choice', 10), ('others', 8)]                 132  \n",
       "4         [('people', 6), ('person', 6), ('act', 5)]                  67  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../../data/train_tokenized.csv\") # Palash's file\n",
    "data.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-200')\n",
    "glove_dict = glove_vectors.key_to_index\n",
    "len(glove_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_emb_weights = glove_vectors.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['list_lower_word_token_nltk'] = data['word_token_nltk'].apply(eval)\n",
    "data['list_lower_word_token_nltk'] = data['list_lower_word_token_nltk'].apply(lambda l: [e.lower() for e in l if e not in ('', ' ')])\n",
    "data['num_valid_token'] = data['list_lower_word_token_nltk'].apply(lambda x: len([c for c in x if c in glove_dict]))\n",
    "data['num_all_token'] = data['list_lower_word_token_nltk'].apply(len)\n",
    "data['unrecgonized_tokens'] = data['list_lower_word_token_nltk'].apply(lambda x: [c for c in x if c not in glove_dict])\n",
    "data[\"word_token_nltk_idx\"] = data['list_lower_word_token_nltk'].apply(\n",
    "    lambda x: [glove_vectors.key_to_index[c]  if c in glove_dict else 400000 for c in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [41, 269, 12, 543, 54, 2161, 25, 2741, 22, 163...\n",
       "1       [61, 7, 747, 14, 7, 511, 81, 33, 4, 886, 20, 8...\n",
       "2       [8607, 1, 2965, 83, 6479, 511, 0, 164, 527, 3,...\n",
       "3       [0, 254, 79, 6, 214, 14, 61, 81, 399, 4961, 2,...\n",
       "4       [357, 743, 3, 20524, 86, 1433, 6, 68, 69, 86, ...\n",
       "                              ...                        \n",
       "3906    [41, 733, 622, 28319, 6, 905, 10, 631, 95, 91,...\n",
       "3907    [500, 1485, 1, 543, 88, 36, 33, 4, 4117, 17, 6...\n",
       "3908    [28, 7, 747, 14, 7, 1019, 10, 81, 4, 88, 392, ...\n",
       "3909    [109, 69, 10027, 17, 4324, 28833, 9, 8643, 1, ...\n",
       "3910    [88, 81, 269, 12, 1899, 14, 0, 444, 873, 10, 6...\n",
       "Name: word_token_nltk_idx, Length: 3911, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"word_token_nltk_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.5, 4.5, 4.5, 4.5, 4.0, 5.0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_cols = [\"cohesion\",\"syntax\",\"vocabulary\", \n",
    "    \"phraseology\", \"grammar\", \"conventions\"]\n",
    "data.loc[3, task_cols].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid, df_test = np.split(\n",
    "    data[[\"text_id\", \"full_text\", \"word_token_nltk_idx\"] + task_cols].sample(\n",
    "        frac=1, random_state=42), \n",
    "    [int(.8*len(data)), int(.9*len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>word_token_nltk_idx</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1552</th>\n",
       "      <td>772D27D400BB</td>\n",
       "      <td>It god to have a possitive attitude when you d...</td>\n",
       "      <td>[20, 1533, 4, 33, 7, 400000, 4191, 61, 81, 88,...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>9E8F3C6405CA</td>\n",
       "      <td>Why do people ask more then one person for adv...</td>\n",
       "      <td>[738, 88, 69, 1712, 56, 127, 48, 899, 10, 3240...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>948771F795EB</td>\n",
       "      <td>We accomplish more when we are active, and are...</td>\n",
       "      <td>[53, 9749, 56, 61, 53, 32, 1546, 1, 5, 32, 690...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3856</th>\n",
       "      <td>FE14D7378CFB</td>\n",
       "      <td>Do you agree or disagree about imagination bei...</td>\n",
       "      <td>[88, 81, 2137, 46, 10027, 59, 9201, 134, 56, 4...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>7AAE019F70D6</td>\n",
       "      <td>I disagree with the principal saying that all ...</td>\n",
       "      <td>[41, 10027, 17, 0, 2965, 345, 12, 64, 1813, 18...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text_id                                          full_text  \\\n",
       "1552  772D27D400BB  It god to have a possitive attitude when you d...   \n",
       "2114  9E8F3C6405CA  Why do people ask more then one person for adv...   \n",
       "1965  948771F795EB  We accomplish more when we are active, and are...   \n",
       "3856  FE14D7378CFB  Do you agree or disagree about imagination bei...   \n",
       "1610  7AAE019F70D6  I disagree with the principal saying that all ...   \n",
       "\n",
       "                                    word_token_nltk_idx  cohesion  syntax  \\\n",
       "1552  [20, 1533, 4, 33, 7, 400000, 4191, 61, 81, 88,...       3.0     2.5   \n",
       "2114  [738, 88, 69, 1712, 56, 127, 48, 899, 10, 3240...       3.0     2.0   \n",
       "1965  [53, 9749, 56, 61, 53, 32, 1546, 1, 5, 32, 690...       4.0     4.0   \n",
       "3856  [88, 81, 2137, 46, 10027, 59, 9201, 134, 56, 4...       3.0     3.0   \n",
       "1610  [41, 10027, 17, 0, 2965, 345, 12, 64, 1813, 18...       3.5     3.5   \n",
       "\n",
       "      vocabulary  phraseology  grammar  conventions  \n",
       "1552         2.5          2.0      2.0          2.0  \n",
       "2114         3.0          3.5      3.0          3.0  \n",
       "1965         3.0          4.0      4.0          4.0  \n",
       "3856         3.5          3.0      3.5          3.5  \n",
       "1610         3.5          3.5      3.0          3.5  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = EssayDataset(df_train, \"word_token_nltk_idx\", task_cols)\n",
    "dl_train = torch.utils.data.DataLoader(\n",
    "    ds_train, batch_size=BATCH_SIZE, \n",
    "    shuffle=True, collate_fn=essay_collate_fn)\n",
    "ds_valid = EssayDataset(df_valid, \"word_token_nltk_idx\", task_cols)\n",
    "dl_valid = torch.utils.data.DataLoader(\n",
    "    ds_valid, batch_size=BATCH_SIZE, \n",
    "    shuffle=True, collate_fn=essay_collate_fn)\n",
    "ds_test = EssayDataset(df_test, \"word_token_nltk_idx\", task_cols)\n",
    "dl_test = torch.utils.data.DataLoader(\n",
    "    ds_test, batch_size=BATCH_SIZE, \n",
    "    shuffle=True, collate_fn=essay_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gru import GRUGrader\n",
    "from trainer import train_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_sizes = [32, 64, 128]\n",
    "gru_depth = [4, 6, 8]\n",
    "bidirectional = [True, False]\n",
    "decoder_depth = [2,3,4]\n",
    "decoder_size = {\n",
    "    2: [256, 64],\n",
    "    3: [256, 512, 64],\n",
    "    4: [256, 512, 128, 64]\n",
    "}\n",
    "\n",
    "model_versions = {\n",
    "    # Default\n",
    "    \"gru_64_6__bidirectiona_True__decoder_3\":{\n",
    "        \"gru_size\":64,\n",
    "        \"gru_num_layer\":6,\n",
    "        \"bidirectional\":True,\n",
    "        \"decoder_depth\":3,\n",
    "        \"decoder_size\":[256, 512, 64]\n",
    "    },\n",
    "    # Experiment with gru size\n",
    "    \"gru_32_6__bidirectiona_True__decoder_3\":{\n",
    "        \"gru_size\":32,\n",
    "        \"gru_num_layer\":6,\n",
    "        \"bidirectional\":True,\n",
    "        \"decoder_depth\":3,\n",
    "        \"decoder_size\":[256, 512, 64]\n",
    "    },\n",
    "    \"gru_128_6__bidirectiona_True__decoder_3\":{\n",
    "        \"gru_size\":128,\n",
    "        \"gru_num_layer\":6,\n",
    "        \"bidirectional\":True,\n",
    "        \"decoder_depth\":3,\n",
    "        \"decoder_size\":[256, 512, 64]\n",
    "    },\n",
    "    # Experiment with gru depth\n",
    "    \"gru_64_4__bidirectiona_True__decoder_3\":{\n",
    "        \"gru_size\":64,\n",
    "        \"gru_num_layer\":4,\n",
    "        \"bidirectional\":True,\n",
    "        \"decoder_depth\":3,\n",
    "        \"decoder_size\":[256, 512, 64]\n",
    "    },\n",
    "    \"gru_64_8__bidirectiona_True__decoder_3\":{\n",
    "        \"gru_size\":64,\n",
    "        \"gru_num_layer\":8,\n",
    "        \"bidirectional\":True,\n",
    "        \"decoder_depth\":3,\n",
    "        \"decoder_size\":[256, 512, 64]\n",
    "    },\n",
    "    # Experiment with bidirectional = False\n",
    "    \"gru_64_6__bidirectiona_False__decoder_3\":{\n",
    "        \"gru_size\":64,\n",
    "        \"gru_num_layer\":6,\n",
    "        \"bidirectional\":False,\n",
    "        \"decoder_depth\":3,\n",
    "        \"decoder_size\":[256, 512, 64]\n",
    "    },\n",
    "    # Experiment with decoder depth\n",
    "    \"gru_64_6__bidirectiona_True__decoder_2\":{\n",
    "        \"gru_size\":64,\n",
    "        \"gru_num_layer\":6,\n",
    "        \"bidirectional\":True,\n",
    "        \"decoder_depth\":2,\n",
    "        \"decoder_size\":[256,64]\n",
    "    },\n",
    "    \"gru_64_6__bidirectiona_True__decoder_4\":{\n",
    "        \"gru_size\":64,\n",
    "        \"gru_num_layer\":6,\n",
    "        \"bidirectional\":True,\n",
    "        \"decoder_depth\":4,\n",
    "        \"decoder_size\":[256, 512, 128, 64]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, params in model_versions.items():\n",
    "    gru_grader = GRUGrader(gensim_emb_weights=gensim_emb_weights, **params)\n",
    "    best_model = train_grader(dl_train, dl_valid, \n",
    "        model=gru_grader,\n",
    "        model_name=name,\n",
    "        save_path=\"drive/MyDrive/data/cs7641/rnn_models\"\n",
    "        loss_func=torch.nn.MSELoss(),\n",
    "        opt=torch.optim.Adam(gru_grader.parameters(), lr=0.001),\n",
    "        epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cs7641project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0347152bab7395cf6d4d53b744fcb402ec7f725dba3cd9225265b4a9dcbbfed9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
